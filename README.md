# SpiderFoot Fetcher

[![Go Report Card](https://goreportcard.com/badge/github.com/luhtaf/spiderfoot-fetcher)](https://goreportcard.com/report/github.com/luhtaf/spiderfoot-fetcher)
[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://golang.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://github.com/luhtaf/spiderfoot-fetcher)
[![Coverage](https://img.shields.io/badge/coverage-85%25-green.svg)](https://github.com/luhtaf/spiderfoot-fetcher)

> High-performance concurrent pipeline for processing SpiderFoot scan results with CVE enrichment and CISA data integration.

## üöÄ Features

- **üîÑ 3-Stage Concurrent Pipeline**: Reader ‚Üí Parser ‚Üí Indexer with configurable worker pools
- **‚ö° High Performance**: Concurrent processing with non-blocking channels
- **üõ°Ô∏è CVE Enrichment**: Automatic CVE scoring with CISA KEV and EPSS integration
- **üìä Real-time Monitoring**: Suricata-style performance metrics and statistics
- **üîç Smart Error Handling**: Detailed error logging with record traceability
- **‚è∞ Safe Timestamp Management**: Prevents duplicate processing during concurrent runs
- **üìà Performance Profiling**: Built-in pprof support for optimization

## üìã Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Operation Modes](#operation-modes)
- [Organization Data](#organization-data)
- [Pipeline Architecture](#pipeline-architecture)
- [CVE Enrichment](#cve-enrichment)
- [Performance Monitoring](#performance-monitoring)
- [Error Handling](#error-handling)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

## üõ†Ô∏è Installation

### Prerequisites
- Go 1.21 or higher
- SQLite3 (SpiderFoot database)
- Elasticsearch 7.x/8.x cluster
- Access to CVE data and EPSS indices

### Install from Source
```bash
git clone https://github.com/luhtaf/spiderfoot-fetcher.git
cd spiderfoot-fetcher
go mod tidy
go build -o spiderfoot-fetcher
```

### Docker Installation
```bash
docker pull luhtaf/spiderfoot-fetcher:latest
```

## üöÄ Quick Start

### 1. Configure the Pipeline
```bash
cp config.yaml.example config.yaml
# Edit config.yaml with your settings
```

### 2. Choose Operation Mode

The application supports two distinct operation modes:

#### üîÑ Pipeline Mode (Default)
Processes new SpiderFoot scan results with full enrichment:
```bash
# Command line override (recommended)
./spiderfoot-fetcher pipeline

# Or set mode in config.yaml
app:
  mode: "pipeline"  # Process new scan results
./spiderfoot-fetcher
```

#### üîß Migration Mode  
Updates existing Elasticsearch records with new enrichment data:
```bash
# Command line override (recommended)
./spiderfoot-fetcher migrate

# Or set mode in config.yaml
app:
  mode: "migration"  # Update existing records
./spiderfoot-fetcher
```

### 3. Command Line Interface

#### Available Commands
```bash
# Show help
./spiderfoot-fetcher help

# Run migration (update existing records)
./spiderfoot-fetcher migrate

# Run pipeline (process new records)  
./spiderfoot-fetcher pipeline

# Use config.yaml mode setting
./spiderfoot-fetcher
```

#### Development vs Production
```bash
# Development mode (dry run - doesn't index to Elasticsearch)
# Edit config.yaml: app.type = "development"
./spiderfoot-fetcher migrate

# Production mode (indexes to Elasticsearch)
# Edit config.yaml: app.type = "production"
./spiderfoot-fetcher migrate
```

### 4. Schedule for Production
```bash
# Pipeline: Hourly processing of new scan results
0 * * * * /path/to/spiderfoot-fetcher pipeline >> /var/log/spiderfoot-pipeline.log 2>&1

# Migration: Weekly refresh of CVE/EPSS data (Sunday 2 AM)
0 2 * * 0 /path/to/spiderfoot-fetcher migrate >> /var/log/spiderfoot-migration.log 2>&1

# Or use systemd timer for pipeline
sudo systemctl enable --now spiderfoot-fetcher.timer
```

### 4. Monitor Performance
```bash
# Real-time stats
tail -f pipeline_stats.json

# Error monitoring
tail -f error.log

# Performance profiling
go tool pprof http://localhost:6060/debug/pprof/profile
```

## ‚öôÔ∏è Configuration

The pipeline is configured via `config.yaml`:

```yaml
# Database Configuration
database:
  path: "spiderfoot.db"
  
# Pipeline Workers Configuration
workers:
  reader: 2      # SQL reader workers
  parser: 4      # Data parser workers  
  indexer: 3     # Elasticsearch indexer workers

# Batch Configuration
batch:
  size: 100      # Records per batch

# Elasticsearch Configuration
elasticsearch:
  url: "http://localhost:9200"
  username: "elastic"
  password: "changeme"
  verify_certs: false
  index: "spiderfoot"
  cve_index: "go-list-cve-*"
  epss_index: "epss-scores"

# Application Configuration
# Note: Operation mode is specified via command line arguments
app:
  type: "development"  # "production" for live indexing
  version: 2
  timestamp_file: "timestamp_cron.txt"
  fallback_hours: 12
  error_log: "error.log"
  organization_data: "organization_data.csv"

# Statistics Configuration
stats:
  enabled: true
  interval: 30s
  file: "pipeline_stats.json"
```

### Environment Variables
```bash
export SPIDERFOOT_DB_PATH="/path/to/spiderfoot.db"
export ELASTICSEARCH_URL="https://elasticsearch:9200"
export ELASTICSEARCH_USERNAME="elastic"
export ELASTICSEARCH_PASSWORD="your-password"
```

## üîÑ Operation Modes

### Pipeline Mode
**Purpose**: Process new SpiderFoot scan results from SQLite database
**Use Case**: Regular data ingestion and enrichment

```bash
# Run pipeline mode
./spiderfoot-fetcher pipeline
```

**Features**:
- ‚úÖ Reads new records based on timestamp tracking
- ‚úÖ 3-stage concurrent processing (Reader ‚Üí Parser ‚Üí Indexer)
- ‚úÖ CVE enrichment with CISA and EPSS data
- ‚úÖ Organization data mapping from CSV
- ‚úÖ Bulk indexing for high performance
- ‚úÖ Prevents duplicate processing

**Best for**: 
- Scheduled cron jobs (hourly/daily)
- Continuous data ingestion
- Processing fresh SpiderFoot scans

### Migration Mode
**Purpose**: Update existing Elasticsearch records with new enrichment data
**Use Case**: Applying new features to historical data

```bash
# Run migration mode (updates records with version < current_version)
./spiderfoot-fetcher migrate
```

**Features**:
- ‚úÖ Uses Scroll API for large dataset processing
- ‚úÖ Bulk update operations (500 records/batch)
- ‚úÖ Progress bar with ETA calculations
- ‚úÖ Fetches latest CISA and EPSS data
- ‚úÖ Updates organization subsektor mappings
- ‚úÖ Version-based targeting
- ‚úÖ Graceful cancellation support

**Best for**:
- One-time data migrations
- Applying new enrichment to existing records
- Version upgrades and schema updates

**Migration Process**:
1. Counts total records to migrate
2. Uses Scroll API to process in batches of 5000
3. Updates records in bulk chunks of 500
4. Shows real-time progress with speed/ETA
5. Updates version number to prevent re-processing

### üîÑ Why Migration is Important

**CVE and EPSS Data is Dynamic**:
CVE scores and EPSS (Exploit Prediction Scoring System) data change frequently:

```
Day 1: CVE-2023-45802 ‚Üí Score: 6.0, EPSS: 0.02
Day 2: CVE-2023-45802 ‚Üí Score: 7.5, EPSS: 0.85  ‚ö†Ô∏è Higher risk!
```

**The Problem with Static Records**:
- üìä **Pipeline Mode**: Only processes NEW SpiderFoot scan results
- üìù **Elasticsearch Records**: Remain static after initial indexing
- ‚è∞ **CVE/EPSS Updates**: Happen daily but don't automatically update existing records

**Solution - Regular Migration**:
```bash
# Weekly migration to refresh CVE/EPSS data (recommended)
0 2 * * 0 /path/to/spiderfoot-fetcher migrate  # Sunday 2 AM

# Manual migration when needed
./spiderfoot-fetcher migrate

# Increment version in config.yaml to force refresh
app:
  version: 7  # Increment when you need to refresh all data
```

**Real-world Example**:
```
Jan 1: Record created with CVE-2023-45802, Score: 6.0 (MEDIUM)
Jan 5: CVE database updated, same CVE now Score: 7.5 (HIGH)
       ‚Üí Your Elasticsearch record still shows 6.0 ‚ùå
Jan 7: Run migration ‚Üí Record updated to 7.5 ‚úÖ
```

**Migration Triggers**:
- üîÑ Weekly/monthly data refresh
- üö® After major CVE database updates
- üìà When EPSS scores change significantly
- üÜï Adding new enrichment features

## üè¢ Organization Data

### CSV Format
Create `organization_data.csv` with organization to subsektor mapping:

```csv
organisasi,subsektor
Bank Bni,Perbankan
Bank Bri,Perbankan
Rumah Sakit Siloam Jakarta,Kesehatan
Telkom Indonesia,Telekomunikasi
Pln,Energi
Garuda Indonesia,Transportasi
Universitas Indonesia,Pendidikan
```

### Configuration
```yaml
app:
  organization_data: "organization_data.csv"  # Path to CSV file
```

### Behavior
- **Automatic Loading**: CSV loaded at startup with logging
- **Graceful Fallback**: Missing file results in empty subsektor fields
- **Case Sensitive**: Exact match required between scan name and CSV data
- **Flexible Format**: Support for spaces and special characters in names

## üì¶ Deployment Patterns

### üïê Cron Job (Recommended)
```bash
# Every hour at minute 0
0 * * * * /opt/spiderfoot-fetcher/spiderfoot-fetcher >> /var/log/spiderfoot.log 2>&1

# Every 30 minutes
*/30 * * * * /opt/spiderfoot-fetcher/spiderfoot-fetcher

# Daily at 2 AM with 24-hour fallback
0 2 * * * /opt/spiderfoot-fetcher/spiderfoot-fetcher
```

### üê≥ Docker Deployment
```bash
# One-time execution
docker run --rm -v /path/to/config:/config luhtaf/spiderfoot-fetcher

# With cron in container
docker run -d --name spiderfoot-cron \
  -v /path/to/config:/config \
  -v /path/to/db:/data \
  luhtaf/spiderfoot-fetcher:latest
```

### ‚öôÔ∏è Systemd Service + Timer
```ini
# /etc/systemd/system/spiderfoot-fetcher.service
[Unit]
Description=SpiderFoot to Elasticsearch Pipeline
After=network.target

[Service]
Type=oneshot
User=spiderfoot
WorkingDirectory=/opt/spiderfoot-fetcher
ExecStart=/opt/spiderfoot-fetcher/spiderfoot-fetcher
StandardOutput=journal
StandardError=journal

# /etc/systemd/system/spiderfoot-fetcher.timer
[Unit]
Description=Run SpiderFoot Pipeline every hour
Requires=spiderfoot-fetcher.service

[Timer]
OnCalendar=hourly
Persistent=true

[Install]
WantedBy=timers.target
```

### üîÑ CI/CD Integration
```yaml
# GitHub Actions example
name: SpiderFoot Data Pipeline
on:
  schedule:
    - cron: '0 */2 * * *'  # Every 2 hours
jobs:
  pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Pipeline
        run: ./spiderfoot-fetcher
```

## üèóÔ∏è Pipeline Architecture

### üîÑ Short-Lived Worker Model

This pipeline uses a **short-lived worker architecture** designed for batch processing:

- **üìÖ Batch-Oriented**: Processes data between timestamps (e.g., last 12 hours)
- **‚è±Ô∏è Finite Execution**: Workers finish when no more data to process
- **üîÑ Cron-Style**: Designed to run periodically (via cron/scheduler)
- **üíæ State Persistence**: Saves timestamp for next run continuity

**vs Long-Running Workers:**
| Aspect | Short-Lived (This Pipeline) | Long-Running Workers ([nvd-fetcher](https://github.com/luhtaf/nvd-fetcher)) |
|--------|----------------------------|---------------------|
| **Lifecycle** | Start ‚Üí Process ‚Üí Exit | Start ‚Üí Listen Forever |
| **Worker Pattern** | Spawn per batch ‚Üí Exit when done | 1000+ workers always listening |
| **Processing Model** | Batch-oriented (100 records/batch) | Per-item (1 CVE/worker) |
| **Memory Profile** | Memory efficient (periodic) | CPU efficient (continuous) |
| **Use Case** | ETL jobs, scheduled processing | Stream processing, real-time |
| **Resource Usage** | Periodic, bounded | Continuous, high throughput |
| **Failure Recovery** | Restart from last timestamp | In-memory state loss |
| **Concurrency** | Finite workers per stage | Massive parallelism (1000+ workers) |
| **Deployment** | Cron job, batch scheduler | Always-on service |

### Stage 1: Reader Workers (Short-Lived)
- **Purpose**: Extract records from SpiderFoot SQLite database
- **Lifecycle**: Query database ‚Üí Send to channel ‚Üí **Exit when no more records**
- **Pattern**: `for { query() ‚Üí send() ‚Üí offset++ }` until empty result
- **Concurrency**: Multiple workers with offset-based pagination (Worker 1: offset 0, Worker 2: offset 100, etc.)
- **Completion**: Workers naturally terminate when their query returns empty
- **Safety**: Timestamp range isolation (`last_run < timestamp <= now`)

### Stage 2: Parser Workers (Channel-Driven)
- **Purpose**: Parse and enrich scan data until channel closes
- **Lifecycle**: `for record := range rawChan` ‚Üí **Exit when channel closes**
- **Processing**: 
  - Grok pattern parsing for organization metadata
  - CVE enrichment with CISA KEV and EPSS data (with caching)
  - Data validation and transformation
- **Pattern**: Channel consumer that terminates when upstream closes
- **Intelligence**: Conditional processing based on scan type

### Stage 3: Indexer Workers (Bounded)
- **Purpose**: Index processed data to Elasticsearch until completion
- **Lifecycle**: `for record := range parsedChan` ‚Üí **Exit when channel closes**
- **Features**: 
  - Dynamic index naming with date partitioning
  - Individual document indexing (not bulk)
  - Error resilience with structured logging
- **Pattern**: Channel consumer with finite data set
- **Monitoring**: Per-operation performance tracking

### üîÑ **Short-Lived vs Long-Running Comparison**

**This Pipeline (Short-Lived)**:
```go
// Reader: Query until no more data
for {
    rows := db.Query("... LIMIT 100 OFFSET ?", offset)
    if len(records) == 0 {
        return // üèÅ Worker exits naturally
    }
    // Process batch and increment offset
}

// Parser: Process until channel closes  
for record := range rawChan {
    process(record)
} // üèÅ Worker exits when channel closes
```

**nvd-fetcher (Long-Running)**:
```go
// 1000 workers always listening
for i := 0; i < 1000; i++ {
    go func() {
        for cveTask := range taskChan { // üîÑ Never exits
            process(cveTask)           // 1 CVE per worker
        }
    }()
}
// Workers run forever, waiting for next CVE
```

```mermaid
graph LR
    A[SQLite DB] --> B[Reader Workers]
    B --> C[Raw Channel]
    C --> D[Parser Workers]
    D --> E[Parsed Channel]
    E --> F[Indexer Workers]
    F --> G[Elasticsearch]
    
    H[CVE Index] --> D
    I[Stats Collector] --> J[Metrics File]
    K[Error Logger] --> L[Error File]
```

## üõ°Ô∏è CVE Enrichment

### Automatic CVE Processing
- **Detection**: Identifies `VULNERABILITY_CVE_*` scan types
- **Enrichment Source**: Queries `go-list-cve-*` Elasticsearch indices
- **Caching**: In-memory LRU cache for performance optimization

### CISA KEV Integration
- **CISA Data**: Known Exploited Vulnerabilities catalog
- **Fields Added**:
  ```json
  {
    "hasCisa": true,
    "cisa": {
      "cisaActionDue": "2022-04-15",
      "cisaExploitAdd": "2022-03-25", 
      "cisaRequiredAction": "Apply updates per vendor instructions.",
      "cisaVulnerabilityName": "HP OpenView Network Node Manager RCE"
    }
  }
  ```

### EPSS Integration
- **EPSS Data**: Exploit Prediction Scoring System from `epss-scores` index
- **Fields Added**:
  ```json
  {
    "hasEpss": true,
    "epss": {
      "cve": "CVE-2021-44228",
      "epss": "0.973730000",
      "percentile": "0.999940000", 
      "date": "2024-10-01",
      "timestamp": "2024-10-02T10:00:59.442658735+07:00"
    }
  }
  ```

### Scoring Logic
1. **Prefer CVSS v3.1** over v2.0 when available
2. **Fallback hierarchy**: v3 ‚Üí v2 ‚Üí base score
3. **Severity mapping**: Numeric score to categorical severity
4. **EPSS enrichment**: Adds exploit prediction probability

## üìä Performance Monitoring

### Real-time Statistics
```json
{
  "timestamp": "2025-09-23T10:30:00Z",
  "reader": {
    "records_per_second": 150.5,
    "avg_processing_time_ms": 45.2,
    "active_workers": 2,
    "total_processed": 3010,
    "error_count": 0
  },
  "parser": {
    "records_per_second": 89.3,
    "avg_processing_time_ms": 112.7,
    "active_workers": 4,
    "queue_depth": 234,
    "error_count": 2
  },
  "indexer": {
    "records_per_second": 92.1,
    "avg_processing_time_ms": 67.8,
    "active_workers": 3,
    "error_count": 1
  },
  "uptime": "2h34m12s"
}
```

### Performance Tuning Guide
- **Reader Bottleneck**: Increase database connections or optimize SQL
- **Parser Bottleneck**: Add parser workers or optimize CVE enrichment
- **Indexer Bottleneck**: Tune Elasticsearch bulk settings or add workers
- **Memory Usage**: Monitor queue depths and adjust batch sizes

## üîç Error Handling

### Structured Error Logging
```json
{
  "timestamp": "2025-09-23T10:30:00Z",
  "stage": "parser",
  "record_id": "abc123hash",
  "scan_name": "security_audit_sektor_finance_organisasi_bank_xyz_target_10.0.1.100",
  "error": "CVE enrichment timeout: connection to elasticsearch failed"
}
```

### Error Recovery Strategies
- **Transient Failures**: Automatic retry with exponential backoff
- **Data Validation**: Skip malformed records with detailed logging
- **Resource Constraints**: Circuit breaker pattern for external services
- **Graceful Degradation**: Continue processing without enrichment when CVE service unavailable

## üß™ Testing

### Running Tests
```bash
# Run all tests
go test ./...

# Run tests with coverage
go test -cover ./...

# Run tests with detailed coverage report
go test -coverprofile=coverage.out ./...
go tool cover -html=coverage.out

# Run benchmarks
go test -bench=. ./...

# Run race condition detection
go test -race ./...
```

### Test Categories
- **Unit Tests**: Individual component testing
- **Integration Tests**: Pipeline stage integration
- **Performance Tests**: Benchmark critical paths
- **Mock Tests**: External service simulation

### Test Coverage Goals
- **Overall Coverage**: >85%
- **Critical Path Coverage**: >95%
- **Error Path Coverage**: >80%

## ü§ù Contributing

### Development Setup
```bash
git clone https://github.com/luhtaf/spiderfoot-fetcher.git
cd spiderfoot-fetcher
go mod tidy
make setup
```

### Code Standards
- **Formatting**: `gofmt` and `goimports`
- **Linting**: `golangci-lint run`
- **Testing**: Minimum 85% coverage
- **Documentation**: Godoc for all public APIs

### Pull Request Process
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests (`make test`)
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push branch (`git push origin feature/amazing-feature`)
6. Open Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [SpiderFoot](https://github.com/smicallef/spiderfoot) - Open source intelligence automation
- [Elasticsearch](https://github.com/elastic/elasticsearch) - Search and analytics engine
- [CISA KEV](https://www.cisa.gov/known-exploited-vulnerabilities) - Known Exploited Vulnerabilities catalog

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/luhtaf/spiderfoot-fetcher/issues)
- **Discussions**: [GitHub Discussions](https://github.com/luhtaf/spiderfoot-fetcher/discussions)
- **Security**: security@luhtaf.dev

---

<div align="center">
Made with ‚ù§Ô∏è by <a href="https://github.com/luhtaf">luhtaf</a>
</div>